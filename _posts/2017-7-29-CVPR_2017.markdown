---
layout:     post
title:      "CVPR2017文献笔记"
subtitle:   " \"小记录\""
date:       2017-7-29 12:00:00
author:     "LancerLian"
header-img: "img/post-bg-2015.jpg"
tags:
    - 学术
---




# CVPR2017 论文小记

> 连盛 2017.7.29

> [IMT Lab](http://imt.xmu.edu.cn/index.php) in XMU

这篇博文会简要几篇我感兴趣的CVPR2017文章。更多的像导读/知识点小回顾，没有精力写太多技术细节，若对文章感兴趣，我对每篇文章都会附上arxiv链接，欢迎交流讨论。我的新浪微博：[lancerlian](http://weibo.com/lancer123) 其他的联系方式自己挖掘哈！

## 文章列表

-  [ ***【2017.07.29】***  ▒▒ Fully Convolutional Instance-aware Semantic Segmentation， Yi Li, et al]( #FCIS ) 

-  [ ***【2017.07.29】***  ▒▒ Fine-tuning Convolutional Neural Networks for Biome﻿dical Image Analysis: Actively and Incrementally， Zongwei Zhou, et al]( #CutAnno ) 

-  [ ***【2017.08.07】***  ▒▒ RON: Reverse Connection with Objectness Prior Networks for Object Detection， Tao Kong, et al]( #Ron ) 



## To do list

- [3D生物医学图像分割](http://mp.weixin.qq.com/s/o_idYS-BTuT460t0MzHH9w)

- [特征金字塔网络FPN](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650729285&idx=3&sn=ae6d28f7423b24bf1f63143af52c79d8&chksm=871b2f3bb06ca62d6e7c6e6db251f754d27b7cd36a9aea7f86fa073f8e710a93924fbb6a4535&scene=21#wechat_redirect)

- [密集连接卷积网络](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650729318&idx=2&sn=b8f8fe9d1ba92a0c95581fa813791d9e&chksm=871b2f18b06ca60ed684a9d57547d02651d2e6372aa24e77df504ceafbf7a8ba06dbc30f188c&scene=21#wechat_redirect) 

- [基于视频的无监督深度和车辆运动估计](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650729371&idx=2&sn=5300a69e564fd97726e40484714e4918&chksm=871b2fe5b06ca6f3c2831c0adb1718bfd1c061d7d6383651975bc9e8023233839ba32e258322&scene=21#wechat_redirect) 

- [用于单目图像车辆3D检测的多任务网络](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650729264&idx=2&sn=2080f22fe20a5df42c72cb4fe7563666&chksm=871b2f4eb06ca6580eab59ff756c2bbec7e8b7c9dfdcd148d6e15f0dedcac268147d30b36b71&scene=21#wechat_redirect)


<br /> <br /> <br /> <br /> <br /> <br /> 





------------------------------------------------------------------------------------------


<div id="FCIS"> </div>

## ***【2017.07.29】***  ▒▒ Fully Convolutional Instance-aware Semantic Segmentation

这篇论文的主要工作是提出了第一个 *全卷积-端到端*  的物体 **实例分割** 解决方案。

论文链接： [arxiv](https://arxiv.org/abs/1611.07709)， 代码链接： [github](https://github.com/daijifeng001/TA-FCN) ，参考 [论文解析](https://mp.weixin.qq.com/s/cANlqQAI-A2mC9vnd3imQA)

**FCN**被广泛用于大多数**语义分割**任务中。它在网络结构中只使用卷积操作，输出结果的通道个数和待分类的类别个数相同。后接一个 softmax 操作来实现每个像素的类别训练。
**物体实例分割**（instance aware segment）有别于语义分割。在语义分割中，同一类的物体并不区分彼此，而是统一标记为同一类。但物体分割需要区分每一个独立的个体。如下图：

![语义分割与物体分割](https://ws1.sinaimg.cn/large/6c0cac2bgy1fi1287qggsj20i903zjur.jpg)

上图的示例可以看出两个任务的区别。左图中的五只羊，在语义分割任务中（中图），被赋予了同一种类别标签。而在物体分割中（右图），每只羊都被赋予了不同的类别。

在一张图像中，待分割的物体个数是不定的，每个物体标记一个类别的话，这张图像的类别个数也是不定的，导致输出的通道个数也无法保持恒定，所以不能直接套用 FCN 的端到端训练框架。

因此，一个直接的想法是，先得到每个物体的检测框，在每个检测框内，再去提取物体的分割结果。这样可以避免类别个数不定的问题。比如，在 faster rcnn 的框架中，提取 ROI 之后，对每个 ROI 区域多加一路物体分割的分支。

这种方法虽然可行，但留有一个潜在的问题：label 的不稳定。想象一下有两个人（A，B）离得很近，以至于每个人的检测框都不得不包含一些另一个人的区域。当我们关注 A 时，B 被引入的部分会标记为背景；相反当我们关注 B 时，这部分会被标记为前景。

为了解决上述问题，本文引用了一种 Instance-sensitive score maps 的方法（首先在 Instance-sensitive Fully Convolutional Networks 一文中被提出），简单却有效的实现了端到端的物体分割训练。

具体的作法是：

将一个 object 的候选框分为 NxN 的格子，每个格子的 feature 来自不同通道的 feature map。

![](https://ws1.sinaimg.cn/large/6c0cac2bgy1fi12bewgc8j20ho060td5.jpg)

以上图为例，可以认为，将物体分割的输出分成了 9 个 channel，分别学习 object 的左上，上，右上，….. 右下等 9 个边界。

这种改变将物体从一个整体打散成为 9 个部分，从而在任何一张 feature map 上，两个相邻的物体的 label 不再连在一起（feature map 1 代表物体的左上边界，可以看到两个人的左上边界并没有连在一起），因此，在每张 feature map 上，两人都是可区分的。

打个比喻，假设本来我们只有一个 person 类别，两个人如果肩并肩紧挨着站在一起，则无法区分彼此。如果我们划分了左手，右手，中心躯干等三个类别，用三张独立的 feature map 代表。那么在每张 feature map 上两个人都是可区分的。当我们需要判断某个候选框内有没有人时，只需要对应的去左手，右手，中心躯干的 feature map 上分别去对应的区域拼在一起，看能不能拼成一个完整的人体即可。

借用这个方法，本文提出了一个物体分割端到端训练的框架，如上图所示，使用 region proposal 网络提供物体分割的 ROI，对每个 ROI 区域，应用上述方法，得到物体分割的结果。

文章中还有一些具体的训练细节，不过这里不再占用篇幅赘述。本文最大的价值在于，第一个提出了在物体分割中可以端到端训练的框架，是继 FCN 之后分割领域的又一个重要进展。

<br /> <br /> <br /> <br />



<div id="CutAnno"> </div>

##***【2017.07.29】***  ▒▒ Fine-tuning Convolutional Neural Networks for Biome﻿dical Image Analysis: Actively and Incrementally
[文章链接](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Fine-Tuning_Convolutional_Neural_CVPR_2017_paper.pdf )

[参考链接](https://mp.weixin.qq.com/s/5E5ZiyjZLHCMIBxk6bZF7Q)

<br /> <br /> <br /> <br />



<div id="Ron"> </div>

##***【2017.07.29】***  ▒▒ RON: Reverse Connection with Objectness Prior Networks for Object Detection， Tao Kong, et al

这篇文章是清华大学孔涛等人的工作。他们尝试将目标检测中的 Regina-based（RCNN系列） 和 Region-free的方法（SSD,YOLO等）的方法结合起来，保留各自的优点，同时解决两个系列方法的缺点，提出了一个有效、高效的通用对象检测框架 Ron。研究设计了反向连接，使网络能够检测多层 CNN 中的对象；提出了 objectness prior 来引导目标对象搜索；利用多任务损失函数优化整个网络，这样网络就能直接预测最终检测结果。在测试中，RON 实现了最先进的对象检测性能。文章被收录于CVPR 2017。

文章相关地址： [文章arxiv链接](https://arxiv.org/abs/1707.01691)，[新智元解析](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652000668&idx=4&sn=1d2605131a3d1b23a1c3a71f3ad821a6&chksm=f121256dc656ac7b53f90da0a77e3571576e96496740150e26d372612d90ad21794bd9b03b3d&scene=0#rd) 。

目前基于深度模型的目标检测框架可以分为两个主要的类别：**region-based**（rcnn系列） 和 **region-free**（ssd，yolo等）。

**基于区域的方法**将对象检测任务分为两个子问题：第一阶段，将一个专用的候选区域生成网络（region proposal generation network）嫁接到深度卷积神经网络（CNN）上；然后，在第二阶段，一个区域性的子网（region-wise subnetwork）被设计来分类和改进这些候选框。使用非常深的 CNN ，Fast R-CNN 工作流程最近在主流对象检测基准上显示出了高精度。
region proposal 阶段可以拒绝大多数背景样本，因此对象检测的搜索空间大大减少。多阶段训练过程一般是开发用于区域候选生成和后检测的联合优化。在 **Fast R-CNN** 中，区域性子网反复评估成千上万个 region proposal，以给出检测分数。在 Fast R-CNN 工作流程下，Faster R-CNN 与检测网络共享全图像卷积特征，实现几乎零成本的 region proposal。最近，**R-FCN** 试图通过添加敏感位置的分数图，使 Faster R-CNN 的 unshared per RoI 计算可共享。然而，R-FCN 仍然需要区域候选网络产生的 region proposal。为了确保检测精度，所有方法都将图像的大小调整到足够大的尺寸。在训练和推理时间内，将图像投喂给深度网络时，会有资源和时间的消耗。例如，使用 Faster R-CNN 预测（将约 5GB GPU 内存用于 VGG-16 网络）每个图像通常需要 0.2 s。

另一个解决方案是**不基于区域**（region-free）的方法。这些方法将对象检测视为一次性（a single shot）问题，使用全卷积网络（FCN），从图像像素一直处理到边界框坐标。这些检测器的主要优点是效率高。从 YOLO 开始，SSD 试图用多层深度 CNN 处理物体检测问题。使用低分辨率输入，SSD 检测器可以获得最先进的检测结果。然而，这些方法的检测精度仍有改进的余地：（a）没有 region proposal，检测器必须在检测模块就要抑制所有的负候选框。这将增加对检测模块进行训练的难度；（b）YOLO 用最顶端的CNN 层检测物体，没有深入探索不同层的检测能力。 SSD 尝试通过添加前一层的结果来提高检测性能。然而，SSD 仍然受困于 small instance 的问题，主要是由于中间层的信息有限。这两个主要瓶颈影响了方法的检测准确性。

####如何结合两者的优势？

为了实现这一目标，研究者关注两个基本问题：（a）多尺度对象定位。各种尺度的物体可能出现在图像的任何位置，因此应考虑成千上万个具有不同位置/尺度/方位的区域。先前的研究表明，多尺度表征将显著改善各种尺度的物体检测。然而，这些方法总是在网络的一层检测到各种尺度的对象。利用研究者提出的反向连接，对象将在其相应的网络尺度上被检测到，这更容易优化；（b）负空间挖掘（Negative space mining）。对象和非对象样本之间的比例严重不平衡。因此，对象检测器应该具有有效的负挖掘策略。为了减少对象搜索空间，研究者在卷积特征图上创建了 objectness prior，并在训练阶段用检测器联合优化。

因此，研究者提出了 RON（Reverse connection with Objectness prior Networks）对象检测框架，将基于区域和不基于区域的方法的优点联系起来。

![](https://ws1.sinaimg.cn/large/6c0cac2bgy1fiey3kvjnvj20jg0fh7db.jpg)
上图是 RON 对象检测总览。给定一张输入图像，网络首先计算骨干网络的特征。然后，（a）添加反向连接；（b）生成 objectness prior；（c）在相应的 CNN 尺度和位置上检测物体。

![](https://ws1.sinaimg.cn/large/6c0cac2bgy1fiey5rmx01j20fa07g0vo.jpg)
上图是生成自特定图像的 objectness prior。在此例中，沙发表现为（a）和（b），棕色的狗表现为（c），斑点狗表现为（d）。在 objectness prior 的引导下，网络生成了检测结果。

![](https://ws1.sinaimg.cn/large/6c0cac2bgy1fiey6t9tyoj2084073mxm.jpg)

![](https://ws1.sinaimg.cn/large/6c0cac2bgy1fiey8w0gmpj20ek06hq40.jpg)

待续